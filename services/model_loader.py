# /hybrid_llm_system/services/model_loader.py
# LLMã¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’æ‹…å½“ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹

from typing import Optional, Any, Union
from llama_cpp import Llama
from domain.schemas import ExpertModel
import torch
from diffusers import DiffusionPipeline, AutoencoderKL

class ModelLoaderService:
    """
    ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒ¯ãƒƒãƒ”ãƒ³ã‚°ï¼‰ã‚’ç®¡ç†ã™ã‚‹
    """
    def __init__(self) -> None:
        self.currently_loaded_expert: Optional[ExpertModel] = None

    def load_expert(self, expert: ExpertModel) -> Union[Llama, DiffusionPipeline]:
        if self.currently_loaded_expert and self.currently_loaded_expert.name == expert.name:
            if expert.instance:
                print(f"âœ… ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã¯æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™ã€‚")
                if isinstance(expert.instance, Llama):
                    print("ğŸ”„ LLMã®å†…éƒ¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã€‚")
                    expert.instance.reset()
                return expert.instance

        if self.currently_loaded_expert:
            self.unload_expert(self.currently_loaded_expert)

        print(f"ğŸ”„ æ€è€ƒå›è·¯ã‚’åˆ‡ã‚Šæ›¿ãˆä¸­... ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")

        try:
            instance: Union[Llama, DiffusionPipeline]
            if expert.chat_format == "diffusion":
                if not expert.model_id:
                    raise ValueError("æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®model_idãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                
                device = "cpu"
                if torch.backends.mps.is_available():
                    device = "mps"
                elif torch.cuda.is_available():
                    device = "cuda"
                print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
                
                vae = AutoencoderKL.from_pretrained(
                    "madebyollin/sdxl-vae-fp16-fix", 
                    torch_dtype=torch.float16
                )
                pipe = DiffusionPipeline.from_pretrained(
                    expert.model_id,
                    vae=vae,
                    torch_dtype=torch.float16,
                    variant="fp16",
                    use_safetensors=True
                )
                instance = pipe.to(device)
                print(f"âœ… æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« '{expert.name}' ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            else:
                if not expert.model_path:
                    raise ValueError("LLMã®model_pathãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                
                # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
                # ã‚¯ãƒ©ãƒƒã‚·ãƒ¥å¯¾ç­–ã¨ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã•ã‚‰ã«å®‰å…¨ãªå€¤ã«å¤‰æ›´
                instance = Llama(
                    model_path=expert.model_path,
                    n_gpu_layers=0,
                    n_ctx=4096,
                    n_batch=256,      # å®‰å®šæ€§ã®ãŸã‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
                    use_mmap=False,   # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã¯å¼•ãç¶šãç„¡åŠ¹
                    verbose=True,     # llama.cppã‹ã‚‰ã®è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–
                    chat_format=expert.chat_format
                )
                # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
                print(f"âœ… LLM '{expert.name}' ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ (ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {expert.chat_format})")

            expert.instance = instance
            expert.is_loaded = True
            self.currently_loaded_expert = expert
            return instance

        except Exception as e:
            expert.instance = None
            expert.is_loaded = False
            self.currently_loaded_expert = None
            raise RuntimeError(f"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")

    def unload_expert(self, expert: Optional[ExpertModel]) -> None:
        if expert and expert.instance:
            print(f"ğŸ§¹ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã—ã¾ã™ã€‚")
            if expert.chat_format == "diffusion":
                if torch.cuda.is_available():
                    del expert.instance
                    torch.cuda.empty_cache()
                elif torch.backends.mps.is_available():
                    del expert.instance
                    torch.mps.empty_cache()

            expert.instance = None
            expert.is_loaded = False
            if self.currently_loaded_expert and self.currently_loaded_expert.name == expert.name:
                self.currently_loaded_expert = None