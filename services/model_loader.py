# /hybrid_llm_system/services/model_loader.py
# LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªã¸ã®ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’æ‹…å½“ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹

from typing import Optional
from llama_cpp import Llama
from domain.schemas import ExpertModel

class ModelLoaderService:
    """
    ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒ¯ãƒƒãƒ”ãƒ³ã‚°ï¼‰ã‚’ç®¡ç†ã™ã‚‹
    """
    def __init__(self) -> None:
        self.currently_loaded_expert: Optional[ExpertModel] = None

    def load_expert(self, expert: ExpertModel) -> Llama:
        # Jamba(chatml)ã®å ´åˆã€å¸¸ã«å†èª­ã¿è¾¼ã¿ã™ã‚‹
        if expert.chat_format == "chatml":
            if self.currently_loaded_expert and self.currently_loaded_expert.name == expert.name:
                print(f"ğŸ”„ Jamba(ChatML)ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã™ã‚‹ãŸã‚ã€å†ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
            self.unload_expert(self.currently_loaded_expert)
        
        # Jambaä»¥å¤–ã§ã€ã™ã§ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å†åˆ©ç”¨
        elif self.currently_loaded_expert and self.currently_loaded_expert.name == expert.name:
            if expert.instance:
                print(f"âœ… ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã¯æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™ã€‚")
                return expert.instance
        
        # åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
        elif self.currently_loaded_expert:
            self.unload_expert(self.currently_loaded_expert)

        print(f"ğŸ”„ æ€è€ƒå›è·¯ã‚’åˆ‡ã‚Šæ›¿ãˆä¸­... ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
        try:
            instance = Llama(
                model_path=expert.model_path,
                n_gpu_layers=0,  # å®‰å®šæ€§ã®ãŸã‚CPUå®Ÿè¡Œã‚’ç¶­æŒ
                n_ctx=8192,
                n_batch=512,
                verbose=False,
                chat_format=expert.chat_format
            )
            expert.instance = instance
            expert.is_loaded = True
            self.currently_loaded_expert = expert
            print(f"âœ… ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ (ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {expert.chat_format})")
            return instance
        except Exception as e:
            expert.instance = None
            expert.is_loaded = False
            self.currently_loaded_expert = None
            raise RuntimeError(f"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")

    def unload_expert(self, expert: Optional[ExpertModel]) -> None:
        if expert and expert.instance:
            print(f"ğŸ§¹ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã—ã¾ã™ã€‚")
            expert.instance = None
            expert.is_loaded = False
            if self.currently_loaded_expert and self.currently_loaded_expert.name == expert.name:
                self.currently_loaded_expert = None