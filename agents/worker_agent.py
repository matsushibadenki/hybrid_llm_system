# /hybrid_llm_system/agents/worker_agent.py
# ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

import os
import uuid
import traceback
from typing import List, Dict, Any, cast
from llama_cpp.llama_types import ChatCompletionRequestMessage
from domain.schemas import SubTask, ExpertModel
from agents.base_agent import BaseAgent
from diffusers import DiffusionPipeline

class WorkerAgent(BaseAgent):
    """
    å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸå˜ä¸€ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    LLMã¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ã«å¯¾å¿œ
    """
    def execute(self, task: SubTask, experts: List[ExpertModel], completed_tasks: Dict[int, SubTask]) -> str:
        """
        ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã€çµæœã‚’æ–‡å­—åˆ—ã§è¿”ã™
        """
        expert = self._find_expert(task.expert_name, experts)
        
        context = ""
        if task.dependencies:
            context += "å…ˆè¡Œã‚¿ã‚¹ã‚¯ã®çµæœ:\n---\n"
            for dep_id in task.dependencies:
                if dep_id in completed_tasks:
                    result = completed_tasks[dep_id].result
                    context += f"ã€ã‚¿ã‚¹ã‚¯{dep_id}ã®çµæœã€‘:\n{result}\n\n"
            context += "---\n"

        prompt = f"{context}ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯:\n{task.description}"
        
        if expert.chat_format == "diffusion":
            return self._generate_image(expert, prompt)
        else:
            messages: List[ChatCompletionRequestMessage] = [
                {"role": "system", "content": expert.system_prompt},
                {"role": "user", "content": prompt}
            ]
            return self._query_llm(expert, messages)

    def _generate_image(self, expert: ExpertModel, prompt: str) -> str:
        """æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹"""
        print(f"ğŸ¨ ç”»åƒç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
        try:
            pipeline_instance = self.model_loader.load_expert(expert)

            pipeline = cast(DiffusionPipeline, pipeline_instance)
            
            # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
            # mypyãŒã“ã®å‘¼ã³å‡ºã—ã‚’ã‚¨ãƒ©ãƒ¼ã¨èª¤èªã™ã‚‹ãŸã‚ã€å‹ãƒã‚§ãƒƒã‚¯ã‚’éƒ¨åˆ†çš„ã«ç„¡è¦–ã•ã›ã‚‹
            image = pipeline(prompt=prompt).images[0]  # type: ignore[operator]
            # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

            output_dir = "output/images"
            os.makedirs(output_dir, exist_ok=True)
            
            filename = f"{uuid.uuid4()}.png"
            filepath = os.path.join(output_dir, filename)
            image.save(filepath)

            print(f"ğŸ–¼ï¸ ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            return f"ç”»åƒã‚’ç”Ÿæˆã—ã€æ¬¡ã®ãƒ‘ã‚¹ã«ä¿å­˜ã—ã¾ã—ãŸ: {filepath}"
        except Exception as e:
            print(f"âŒ ç”»åƒç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            traceback.print_exc()
            return f"ç”»åƒç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}"

    def _find_expert(self, name: str, experts: List[ExpertModel]) -> ExpertModel:
        """æŒ‡å®šã•ã‚ŒãŸåå‰ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã‚‹"""
        for expert in experts:
            if expert.name.lower() == name.lower():
                return expert
        raise ValueError(f"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")