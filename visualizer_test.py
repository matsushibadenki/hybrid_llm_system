# /hybrid_llm_system/visualizer_test.py
# æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«(Stable Diffusion)ã®å‹•ä½œã‚’æœ€å°æ§‹æˆã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

import os
import torch
from dotenv import load_dotenv
from diffusers import DiffusionPipeline, AutoencoderKL
from typing import Optional

def main() -> None:
    """
    æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æœ€å°ç’°å¢ƒãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    print("--- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«(Stable Diffusion)ã®æœ€å°ç’°å¢ƒãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™ ---")
    
    try:
        load_dotenv()
        # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Hugging Faceã®ãƒ¢ãƒ‡ãƒ«IDã‚’å–å¾—
        model_id: Optional[str] = os.getenv("VISUALIZER_MODEL_ID")

        if not model_id or not model_id.strip():
            print("âŒ ã‚¨ãƒ©ãƒ¼: .envãƒ•ã‚¡ã‚¤ãƒ«ã«VISUALIZER_MODEL_IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            print("   (ä¾‹: VISUALIZER_MODEL_ID=\"stabilityai/stable-diffusion-xl-base-1.0\")")
            return
        
        # GGUFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒèª¤ã£ã¦è¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if ".gguf" in model_id.lower():
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: VISUALIZER_MODEL_IDã«ã¯GGUFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã§ã¯ãªãã€Hugging Faceã®ãƒªãƒã‚¸ãƒˆãƒªIDã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            print(f"   ç¾åœ¨ã®å€¤: {model_id}")
            print(f"   æ­£ã—ã„ä¾‹: \"stabilityai/stable-diffusion-xl-base-1.0\"")
            return

        print(f"Hugging Face ãƒ¢ãƒ‡ãƒ«ID: {model_id}")

        # ãƒ‡ãƒã‚¤ã‚¹ã®è‡ªå‹•é¸æŠ (MPS > CUDA > CPU)
        device = "cpu"
        if torch.backends.mps.is_available():
            device = "mps"
        elif torch.cuda.is_available():
            device = "cuda"
        
        print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

        # VAE (Variational Auto Encoder) ã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰
        # ã“ã‚Œã«ã‚ˆã‚Šã€ç‰¹å®šã®ç’°å¢ƒã§ã®é»’ã„ç”»åƒãŒå‡ºåŠ›ã•ã‚Œã‚‹å•é¡Œã‚’å›é¿
        vae = AutoencoderKL.from_pretrained(
            "madebyollin/sdxl-vae-fp16-fix", 
            torch_dtype=torch.float16
        )

        # æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–
        # 'from_pretrained'ã¯ãƒªãƒã‚¸ãƒˆãƒªIDã‚’å…ƒã«Hugging Face Hubã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™
        pipe = DiffusionPipeline.from_pretrained(
            model_id,
            vae=vae,
            torch_dtype=torch.float16,
            variant="fp16",
            use_safetensors=True
        )
        pipe = pipe.to(device)
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸã€‚")

        # ç”»åƒç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = "A cinematic shot of a baby raccoon wearing an intricate italian mafioso suit, saying 'oh no'."

        print(f"\n--- ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™... ---\n{prompt}")
        
        # ç”»åƒã‚’ç”Ÿæˆ
        image = pipe(prompt=prompt).images[0]
        
        print("\n--- ç”»åƒã®ç”Ÿæˆã«æˆåŠŸã—ã¾ã—ãŸï¼ ---")

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir = "output/images"
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        output_path = os.path.join(output_dir, "visualizer_test_output.png")
        image.save(output_path)
        
        print(f"\nğŸ–¼ï¸ ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {os.path.abspath(output_path)}")

    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

    print("\n--- ãƒ†ã‚¹ãƒˆã‚’çµ‚äº†ã—ã¾ã™ ---")

if __name__ == "__main__":
    main()